# Prompt

!!! abstract
    本文的起因是 GitHub 上一位大佬给 Claude3.5 写了一个 Prompt ，把 o1 级别的思维链复刻到 Claude3.5 上。这边留下 GitHub 指路[链接](https://github.com/richards199999/Thinking-Claude)

    所以上网搜集了一些资料整合了一下，记录下来。

所以什么是 Prompt 呢？

Prompt 是一种引导模型生成特定类型输出的人工输入。它通常包含一些指示、上下文或示例，以帮助模型理解用户的需求并生成相应的回答。

我们可以从 GPT 的发展来了解 Prompt 。

**阶段1：**

GPT-1 诞生在 Transformer 初期，是最早期基于 Transformer 架构打造的模型之一，通过 "pretrain + finetune" 的方式，首先让模型在大量未标注的数据上自监督的进行学习，完成预训练，随后在应用时再使用**有监督数据**进行微调，以此让模型可以适用于各种任务。

**阶段2：**

相比 GPT-1，GPT-2 第一次提出了全新的范式，当我们扩大模型规模增加训练数据，让模型在一个称为 WebText 的由数百万个网页组成的数据集上完成预训练后，模型不再需要任何监督数据，就可以完成各类任务。在 OpenAI 的 Blog 中我们可以看到，团队在研究过程中发现，提升模型规模及训练数据的体量，可以让模型在 zero-shot 任务中的效果明显提升。这也被视为 scaling law 的第一次发现。

!!! note "[Scaling Law](./scaling%20law.pdf)"
    语言模型性能与模型大小、数据大小和计算预算之间的关系如下：

    - 模型形状对性能影响不大：在保持总参数数量不变的情况下，改变模型的形状（如宽度、深度等）对性能的影响非常小。对于不同形状的模型，损失只会有几个百分点的变化。

    - 性能随模型大小和数据大小的增加而提高：语言模型的性能随着模型大小、数据大小和计算预算的增加而提高。然而，在某些情况下，增加数据大小可能比增加模型大小更有效。

    - 性能随计算预算的增加而提高：在相同的模型大小和数据大小下，增加计算预算可以提高语言模型的性能。然而，当计算预算增加到一定程度后，进一步增加计算预算可能只会带来微小的性能提升。
    
    - 最优模型大小和计算预算取决于任务和数据集：对于不同的任务和数据集，最优的模型大小和计算预算可能会有所不同。这可能是因为不同的任务和数据集具有不同的复杂性和数据分布。
    
    - 过拟合随模型大小和数据大小的增加而增加：语言模型的过拟合程度随着模型大小和数据大小的增加而增加。为了减轻过拟合，可以使用正则化技术，如 dropout。
    
    - 学习速度随模型大小的增加而降低：语言模型的学习速度随着模型大小的增加而降低。这可能是因为较大的模型需要更多的时间来调整其参数。
    
    - 最优学习率取决于任务和模型结构：最优的学习率可能会因任务和模型结构的不同而有所变化。在实际应用中，需要根据具体情况选择合适的学习率。
    
    - 硬件限制可能影响模型大小和计算预算的选择：实际应用中，硬件限制可能会影响模型大小和计算预算的选择。例如，当计算资源有限时，可能需要选择较小的模型和较低的计算预算。

    然而 Scaling law 论文有一个非常重要的前提条件，就是都是标准的transformer模型结构。如果不是，scaling law是失效了的。原论文有做对比试验，也可以参考这篇论文,[2019年的ACL Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](./Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.pdf)。

**阶段3：**

沿着 GPT-2 增大模型体量和训练数据规模的思路，GPT-3 使用了 570G 的训练数据，达到了 GPT-2 的15倍，参数量更是达到了惊人的 1750B，是 GPT-2 的 116 倍。参数量的提升让模型在各个领域中都展现出了令人惊艳的效果，尤其是在 zero-shot 方面。

**阶段4：**

OpenAI 在 GPT-3 的基础上针对不同场景进行了优化，在 "多轮对话" 的优化中诞生了 "ChatGPT"，随后成为了世界上最火热的话题，也被认为是 AI 市场化的起点。

从领域发展的角度我们可以看到 3 种不同的研发范式：

1. Transformer 之前：有监督训练（train）
2. GPT1：无监督预训练（pre-train） + 有监督微调（finetune）
3. GPT2 & GPT3：无监督预训练（pre-train） + 提示词（Prompt）

---

我们要做的是通过 Prompt 调用大模型的能力去解决问题，让这些能力表现的更精准，而并非把他当成一个知识库。

Sam Altman 在早期采访中也提到，把大模型当成搜索引擎的应用方式是错误的，智能的体现才是大模型的关键。

这里要放一篇[神文链接](https://www.promptingguide.ai/zh)(其实你很难想象一个考试周的人还花时间在这里学 Prompt 到底是什么想法，~~所以我说我之后一定会去看的也是很有道理的~~)

- TO-DO

[ ] 大概会想自己写一个简单的 Prompt ，然后看看效果如何，关于如何写如何优化 Prompt 我想我还是会记录的，所以留一个 TODO 吧。

!!! Info "Reference"
    《一文掌握Prompt：万能框架+优化技巧+常用指标》刘琮玮